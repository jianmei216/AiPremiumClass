1. 使用豆瓣电影评论数据完成文本分类处理：文本预处理，加载、构建词典。（评论得分1～2	表示positive取值：1，评论得分4～5代表negative取值：0）
https://www.kaggle.com/datasets/utmhikari/doubanmovieshortcomments
2. 加载处理后文本构建词典、定义模型、训练、评估、测试。
3. 尝试不同分词工具进行文本分词，观察模型训练结果。

总结：
1. 对电影评分数据进行预处理清洗,只保留star in (1,2,4,5)，非空评论，评论长度>=2个字符的评论数据，
   并将star=1,2的数据标签设置为0差评，star=4,5的设置为1好评。加载清洗后的数据，构建词典并保存。
2. 加载已保存的词典或重新构建词典，模型定义使用LSTM结构，训练集为8.2万数据，训练5轮，batch_size，固定学习率0.001
   词向量特征维度100维，隐藏层大小128个神经元。在CPU上训练大约2个小时，loss从0.6 -> 0.1左右,后面轮次基本没有变化
   在测试集上准确率为90.4%，2条评论结果测试正确。
3. 尝试不同分词工具进行文本分词，观察模型训练结果。2个epoch，batchsize=512
   1) 原生分词：将文本直接按字符切分，不进行任何处理。本语料词典表大小=9311
   2) jieba分词：使用jieba库进行中文分词，能够将文本切分成有意义的词语。本语料的词典表大小=287571
   3) 对比结果：通过对比不同分词工具的效果，我们可以发现jieba分词在处理中文文本时效果更好，能够将文本切分成有意义的词语，
      jieba分词模型训练准确率：(CPU上77.66%，kaggle上90.33%）
      native分词模型训练准确率：(CPU上90.65%，kaggel上91.35%）
      使用两种分词工具，模型训练的准确率有些区别，原生分词更高一些。
